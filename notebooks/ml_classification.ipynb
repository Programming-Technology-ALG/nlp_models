{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = Path('../cache/news_6771.json')\n",
    "with news_path.open(encoding=\"UTF-8\") as f:\n",
    "    all_data = json.load(f).get('catalog')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categories: {'politics', 'gorod', 'culture', 'economics', 'world', 'incidents'}\n",
      "Len of dataset: 4361\n"
     ]
    }
   ],
   "source": [
    "print(f\"All categories: {set(pd.DataFrame(all_data).get('category'))}\")\n",
    "\n",
    "cats = ['culture', 'economics', 'gorod', 'politics']\n",
    "news = [article for article in all_data if article.get('category') in cats]\n",
    "print(f'Len of dataset: {len(news)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My corpus is big enough (60_000 < features) and M1 chip processed some models for 12+ hours.\n",
    "So, aim of so significant preprocessing is features redusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "mystem = Mystem()\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and lemmatize it with pymystem3 and remove stopwords and punctuation symbols from it\n",
    "\n",
    "    * Stemming is used for significant vocabulary reducing. In other case it hase dim > 60_000\n",
    "    * Removed all punctuation simbols\n",
    "    * Removed all numbers\n",
    "    * Removed all small words\n",
    "    * Removed site link (nn ru)\n",
    "    * Removed unknown words\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = mystem.lemmatize(text.lower().replace('ё', 'е'))\n",
    "    \n",
    "    marks = '''\n",
    "    !()-[]{};?@#$%:'\"\\,./^&amp;*_—«»–…’‘“”\n",
    "    '''\n",
    " \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        token = stemmer.stem(token)\n",
    "        if len(token) < 2: continue\n",
    "        for x in token:\n",
    "            if x in marks:\n",
    "                token = token.replace(x, \"\")\n",
    "        new_tokens.append(token)\n",
    "    tokens = new_tokens\n",
    "\n",
    "\n",
    "    blacklist = ['\\n', '\\t', '\\r', ' ', '', *punctuation, *russian_stopwords, *english_stopwords, 'nn', 'ru']\n",
    "    tokens = [token for token in tokens if token not in blacklist and not token.isdigit() and morph.word_is_known(token)]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from typing import Optional\n",
    "\n",
    "class CorpusStructure:\n",
    "    \"\"\"\n",
    "    Structure of corpus\n",
    "    \"\"\"\n",
    "    corpus: list\n",
    "    target: list\n",
    "    vectorizer: CountVectorizer\n",
    "    matrix: Optional[list] = None\n",
    "\n",
    "    def __init__(self, corpus: list, targets: list, vectorizer: CountVectorizer) -> None:\n",
    "        self._corpus = corpus\n",
    "        self._target = targets\n",
    "        self._vectorizer = vectorizer\n",
    "        self._matrix = None\n",
    "\n",
    "    @property\n",
    "    def corpus(self) -> list:\n",
    "        return self._corpus\n",
    "\n",
    "    @property\n",
    "    def target(self) -> list:\n",
    "        return self._target\n",
    "\n",
    "    @property\n",
    "    def corpus_len(self) -> int:\n",
    "        return len(self._corpus)\n",
    "\n",
    "    @property\n",
    "    def target_len(self) -> int:\n",
    "        return len(self._target)\n",
    "\n",
    "    @property\n",
    "    def matrix(self):\n",
    "        if self._matrix == None:\n",
    "            self._matrix = self._vectorizer.transform(self._corpus)\n",
    "        return self._matrix.toarray()\n",
    "\n",
    "    def transform(self):\n",
    "        return self._vectorizer.transform(self._corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "def train_test_partition(corpus: list, targets: list, vectorizer: CountVectorizer, test_size: float = 0.2) -> tuple:\n",
    "    \"\"\"\n",
    "    Function that creates train and test partition\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(corpus, targets, test_size=test_size)\n",
    "    return CorpusStructure(X_train, y_train, vectorizer), CorpusStructure(X_test, y_test, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_corpus(news: list, n: int = -1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Provides preprocessing for corpus\n",
    "    \"\"\"\n",
    "    corpus_text = []\n",
    "    corpus_target = []\n",
    "    n = len(news) if n == -1 else n\n",
    "    if shuffle: random.shuffle(news)\n",
    "    for article in news[:n]:\n",
    "        corpus_text.append(preprocess_text(article.get('text')))\n",
    "        corpus_target.append(article.get('category'))\n",
    "    return corpus_text, corpus_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "train_path = Path('../cache/vectorizer_train.pkl')\n",
    "test_path = Path('../cache/vectorizer_test.pkl')\n",
    "\n",
    "if train_path.exists() and test_path.exists():\n",
    "    train_data = pickle.load(open('../cache/vectorizer_train.pkl', 'rb'))\n",
    "    test_data = pickle.load(open('../cache/vectorizer_test.pkl', 'rb'))\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    corpus_text, corpus_target = get_corpus(news)\n",
    "    vectorizer.fit(corpus_text)\n",
    "\n",
    "    train_data, test_data = train_test_partition(corpus_text, corpus_target, vectorizer, test_size=0.2)\n",
    "\n",
    "    pickle.dump(train_data, open('../cache/vectorizer_train.pkl', 'wb'))\n",
    "    pickle.dump(test_data, open('../cache/vectorizer_test.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stemmer: 35_053 \\\n",
    "Processing time: > 12h \n",
    "\n",
    "With stemmer: 14_566\\\n",
    "Processing time: 0.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14566"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Note</b> \\\n",
    "Removing categories ['world', 'incidents'] did not reduce features len. \\\n",
    "So, I suppose that the my site could use common language style for all news. It will lead to model accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best parameters for models I will use Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\"\"\"Module for grid search and classification of the data\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "models = [\n",
    "    (\n",
    "        'LogisticRegression',\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            'solver'    : ['newton-cg'],\n",
    "            'max_iter'  : [1000]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'MultinomialNB',\n",
    "        MultinomialNB(),\n",
    "        {\n",
    "            'alpha': [0.1]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'LinearSVC',\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'loss'      : ['hinge'],\n",
    "            'max_iter'  : [1000]\n",
    "        }\n",
    "    ),\n",
    "    # (\n",
    "    #     'SGDClassifier',\n",
    "    #     SGDClassifier(),\n",
    "    #     {\n",
    "    #         'penalty'       : ['l1','l2'],\n",
    "    #         'alpha'         : [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000],\n",
    "    #         'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    #         'max_iter'      : [100],\n",
    "    #         'loss'          : ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    #     }\n",
    "    # ),\n",
    "    (\n",
    "        'RandomForestClassifier',\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            'criterion' : ['gini']\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'KNeighborsClassifier',\n",
    "        KNeighborsClassifier(),\n",
    "        {\n",
    "            'weights'    : ['distance'],\n",
    "            'n_neighbors': [9, 10, 11],\n",
    "            'p'          : [2]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'DecisionTreeClassifier',\n",
    "        DecisionTreeClassifier(),\n",
    "        {\n",
    "            'criterion'     : ['gini'],\n",
    "            'max_features'  : ['sqrt']\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(name: str, test: list, predicted: list, target: list):\n",
    "    disp = metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(test, predicted, labels=target))\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(f'{name} Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression in 139.23 seconds. Accuracy: 0.8350515463917526\n",
      "Best parameters: {'max_iter': 1000, 'solver': 'newton-cg'}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "cache_path = Path('../cache')\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for i, (name, model, params) in enumerate(models):\n",
    "    start_time = time.perf_counter()\n",
    "    grid_classifier = GridSearchCV(model,\n",
    "                                    params,\n",
    "                                    cv=10,\n",
    "                                    scoring='accuracy',\n",
    "                                    verbose=0,\n",
    "                                    error_score=0,\n",
    "                                    n_jobs=-1\n",
    "                                    )\n",
    "    grid_classifier.fit(train_data.matrix, train_data.target)\n",
    "    predicted = grid_classifier.predict(test_data.matrix)\n",
    "\n",
    "    accuracy.append(metrics.accuracy_score(test_data.target, predicted))\n",
    "    precision.append(metrics.precision_score(test_data.target, predicted, average='micro', zero_division=0))\n",
    "    recall.append(metrics.recall_score(test_data.target, predicted, average='micro', zero_division=0))\n",
    "    f1.append(metrics.f1_score(test_data.target, predicted, average='micro', zero_division=0))\n",
    "\n",
    "    print(f'Finished {name} in {time.perf_counter() - start_time:.2f} seconds. Accuracy: {accuracy[-1]}', flush=True)\n",
    "    print(f'Best parameters: {grid_classifier.best_params_}')\n",
    "    \n",
    "    # ckpt_name = cache_path / f'{name}_{accuracy}.pkl'\n",
    "    # names = list(cache_path.glob(f'{name}_*.pkl'))\n",
    "    # if len(names) and accuracy > names[-1].split('_')[-1]:\n",
    "    #     with ckpt_name.open() as file_descr:\n",
    "    #         pickle.dump(grid_classifier, file_descr)\n",
    "\n",
    "    plot_confusion_matrix(name, test_data.target, predicted, corpus_target)\n",
    "\n",
    "metrics_frame = pd.DataFrame({\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}, index=[name for name, _, _ in models])\n",
    "\n",
    "metrics_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Finished LogisticRegression in 139.23 seconds. Accuracy: 0.8350515463917526\n",
    "Best parameters: {'max_iter': 1000, 'solver': 'newton-cg'}\n",
    "\n",
    "\n",
    "Finished MultinomialNB in 15.37 seconds. Accuracy: 0.7961053837342497\n",
    "Best parameters: {'alpha': 0.1}\n",
    "Finished RandomForestClassifier in 41.84 seconds. Accuracy: 0.8006872852233677\n",
    "Best parameters: {'criterion': 'gini'}\n",
    "Finished KNeighborsClassifier in 245.95 seconds. Accuracy: 0.7789232531500573\n",
    "Best parameters: {'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
    "Finished DecisionTreeClassifier in 10.31 seconds. Accuracy: 0.5715922107674685\n",
    "Best parameters: {'criterion': 'gini', 'max_features': 'sqrt'}\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8378c2eac6e99e6afd3bc83cd2d86f951c26788ceeb3cc02a7a609063a83c45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
