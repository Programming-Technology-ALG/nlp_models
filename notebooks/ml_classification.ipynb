{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = Path('../cache/news_6771.json')\n",
    "with news_path.open(encoding=\"UTF-8\") as f:\n",
    "    all_data = json.load(f).get('catalog')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categories: {'gorod', 'incidents', 'culture', 'politics', 'world', 'economics'}\n",
      "Len of dataset: 4361\n"
     ]
    }
   ],
   "source": [
    "print(f\"All categories: {set(pd.DataFrame(all_data).get('category'))}\")\n",
    "\n",
    "cats = ['culture', 'economics', 'gorod', 'politics']\n",
    "news = [article for article in all_data if article.get('category') in cats]\n",
    "print(f'Len of dataset: {len(news)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My corpus is big enough (60_000 < features) and M1 chip processed some models for 12+ hours.\n",
    "So, aim of so significant preprocessing is features redusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "mystem = Mystem()\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and lemmatize it with pymystem3 and remove stopwords and punctuation symbols from it\n",
    "\n",
    "    * Stemming is used for significant vocabulary reducing. In other case it hase dim > 60_000\n",
    "    * Removed all punctuation simbols\n",
    "    * Removed all numbers\n",
    "    * Removed all small words\n",
    "    * Removed site link (nn ru)\n",
    "    * Removed unknown words\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = mystem.lemmatize(text.lower().replace('ё', 'е'))\n",
    "    \n",
    "    marks = '''\n",
    "    !()-[]{};?@#$%:'\"\\,./^&amp;*_—«»–…’‘“”\n",
    "    '''\n",
    " \n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        token = stemmer.stem(token)\n",
    "        if len(token) < 2: continue\n",
    "        for x in token:\n",
    "            if x in marks:\n",
    "                token = token.replace(x, \"\")\n",
    "        new_tokens.append(token)\n",
    "    tokens = new_tokens\n",
    "\n",
    "\n",
    "    blacklist = ['\\n', '\\t', '\\r', ' ', '', *punctuation, *russian_stopwords, *english_stopwords, 'nn', 'ru']\n",
    "    tokens = [token for token in tokens if token not in blacklist and not token.isdigit() and morph.word_is_known(token)]\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from typing import Optional\n",
    "\n",
    "class CorpusStructure:\n",
    "    \"\"\"\n",
    "    Structure of corpus\n",
    "    \"\"\"\n",
    "    corpus: list\n",
    "    target: list\n",
    "    vectorizer: CountVectorizer\n",
    "    matrix: Optional[list] = None\n",
    "\n",
    "    def __init__(self, corpus: list, targets: list, vectorizer: CountVectorizer) -> None:\n",
    "        self._corpus = corpus\n",
    "        self._target = targets\n",
    "        self._vectorizer = vectorizer\n",
    "        self._matrix = None\n",
    "\n",
    "    @property\n",
    "    def corpus(self) -> list:\n",
    "        return self._corpus\n",
    "\n",
    "    @property\n",
    "    def target(self) -> list:\n",
    "        return self._target\n",
    "\n",
    "    @property\n",
    "    def corpus_len(self) -> int:\n",
    "        return len(self._corpus)\n",
    "\n",
    "    @property\n",
    "    def target_len(self) -> int:\n",
    "        return len(self._target)\n",
    "\n",
    "    @property\n",
    "    def matrix(self):\n",
    "        if self._matrix == None:\n",
    "            self._matrix = self._vectorizer.transform(self._corpus)\n",
    "        return self._matrix.toarray()\n",
    "\n",
    "    def transform(self):\n",
    "        return self._vectorizer.transform(self._corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "def train_test_partition(corpus: list, targets: list, vectorizer: CountVectorizer, test_size: float = 0.2) -> tuple:\n",
    "    \"\"\"\n",
    "    Function that creates train and test partition\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(corpus, targets, test_size=test_size)\n",
    "    return CorpusStructure(X_train, y_train, vectorizer), CorpusStructure(X_test, y_test, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_corpus(news: list, n: int = -1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Provides preprocessing for corpus\n",
    "    \"\"\"\n",
    "    corpus_text = []\n",
    "    corpus_target = []\n",
    "    n = len(news) if n == -1 else n\n",
    "    if shuffle: random.shuffle(news)\n",
    "    for article in news[:n]:\n",
    "        corpus_text.append(preprocess_text(article.get('text')))\n",
    "        corpus_target.append(article.get('category'))\n",
    "    return corpus_text, corpus_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "train_path = Path('../cache/vectorizer_train.pkl')\n",
    "test_path = Path('../cache/vectorizer_test.pkl')\n",
    "vectorizer_path = Path('../cache/vectorizer.pkl')\n",
    "\n",
    "corpus_text, corpus_target = get_corpus(news)\n",
    "\n",
    "if train_path.exists() and test_path.exists() and vectorizer_path.exists():\n",
    "    train_data = pickle.load(open('../cache/vectorizer_train.pkl', 'rb'))\n",
    "    test_data = pickle.load(open('../cache/vectorizer_test.pkl', 'rb'))\n",
    "    vectorizer = pickle.load(open('../cache/vectorizer.pkl', 'rb'))\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(corpus_text)\n",
    "\n",
    "    train_data, test_data = train_test_partition(corpus_text, corpus_target, vectorizer, test_size=0.2)\n",
    "\n",
    "    pickle.dump(train_data, open('../cache/vectorizer_train.pkl', 'wb'))\n",
    "    pickle.dump(test_data, open('../cache/vectorizer_test.pkl', 'wb'))\n",
    "    pickle.dump(vectorizer, open('../cache/vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stemmer: 35_053 \\\n",
    "Processing time: > 12h \n",
    "\n",
    "With stemmer: 14_566\\\n",
    "Processing time: 0.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Note</b> \\\n",
    "Removing categories ['world', 'incidents'] did not reduce features len. \\\n",
    "So, I suppose that the my site could use common language style for all news. It will lead to model accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best parameters for models I will use Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\"\"\"Module for grid search and classification of the data\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "models = [\n",
    "    (\n",
    "        'LogisticRegression',\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            'solver'    : ['newton-cg'],\n",
    "            'max_iter'  : [1000]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'MultinomialNB',\n",
    "        MultinomialNB(),\n",
    "        {\n",
    "            'alpha': [0.1]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'LinearSVC',\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'loss'      : ['hinge'],\n",
    "            'max_iter'  : [1000]\n",
    "        }\n",
    "    ),\n",
    "    # (\n",
    "    #     'SGDClassifier',\n",
    "    #     SGDClassifier(),\n",
    "    #     {\n",
    "    #         'penalty'       : ['l1','l2'],\n",
    "    #         'alpha'         : [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000],\n",
    "    #         'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    #         'max_iter'      : [100],\n",
    "    #         'loss'          : ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    #     }\n",
    "    # ),\n",
    "    (\n",
    "        'RandomForestClassifier',\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            'criterion' : ['gini']\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'KNeighborsClassifier',\n",
    "        KNeighborsClassifier(),\n",
    "        {\n",
    "            'weights'    : ['distance'],\n",
    "            'n_neighbors': [10],\n",
    "            'p'          : [2]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'DecisionTreeClassifier',\n",
    "        DecisionTreeClassifier(),\n",
    "        {\n",
    "            'criterion'     : ['gini'],\n",
    "            'max_features'  : ['sqrt']\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(name: str, test: list, predicted: list, target: list):\n",
    "    disp = metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(test, predicted, labels=target))\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(f'{name} Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished LogisticRegression in 126.26 seconds. Accuracy: 0.8350515463917526\n",
      "Best parameters: {'max_iter': 1000, 'solver': 'newton-cg'}\n",
      "Finished MultinomialNB in 3.46 seconds. Accuracy: 0.8247422680412371\n",
      "Best parameters: {'alpha': 0.1}\n",
      "Finished LinearSVC in 3.36 seconds. Accuracy: 0.8384879725085911\n",
      "Best parameters: {'loss': 'hinge', 'max_iter': 1000}\n",
      "Finished RandomForestClassifier in 24.40 seconds. Accuracy: 0.7972508591065293\n",
      "Best parameters: {'criterion': 'gini'}\n",
      "Finished KNeighborsClassifier in 5.86 seconds. Accuracy: 0.8029782359679267\n",
      "Best parameters: {'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "Finished DecisionTreeClassifier in 2.61 seconds. Accuracy: 0.5830469644902635\n",
      "Best parameters: {'criterion': 'gini', 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.835052</td>\n",
       "      <td>0.837531</td>\n",
       "      <td>0.835096</td>\n",
       "      <td>0.835867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.824742</td>\n",
       "      <td>0.821487</td>\n",
       "      <td>0.825517</td>\n",
       "      <td>0.821094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.838488</td>\n",
       "      <td>0.837940</td>\n",
       "      <td>0.839205</td>\n",
       "      <td>0.838409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.797251</td>\n",
       "      <td>0.797744</td>\n",
       "      <td>0.795450</td>\n",
       "      <td>0.795627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.802978</td>\n",
       "      <td>0.801267</td>\n",
       "      <td>0.803266</td>\n",
       "      <td>0.798566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.583047</td>\n",
       "      <td>0.584210</td>\n",
       "      <td>0.583919</td>\n",
       "      <td>0.583243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        accuracy  precision    recall        f1\n",
       "LogisticRegression      0.835052   0.837531  0.835096  0.835867\n",
       "MultinomialNB           0.824742   0.821487  0.825517  0.821094\n",
       "LinearSVC               0.838488   0.837940  0.839205  0.838409\n",
       "RandomForestClassifier  0.797251   0.797744  0.795450  0.795627\n",
       "KNeighborsClassifier    0.802978   0.801267  0.803266  0.798566\n",
       "DecisionTreeClassifier  0.583047   0.584210  0.583919  0.583243"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "cache_path = Path('../cache')\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for i, (name, model, params) in enumerate(models):\n",
    "    start_time = time.perf_counter()\n",
    "    grid_classifier = GridSearchCV(model,\n",
    "                                    params,\n",
    "                                    cv=10,\n",
    "                                    scoring='accuracy',\n",
    "                                    verbose=0,\n",
    "                                    error_score=0,\n",
    "                                    n_jobs=-1\n",
    "                                    )\n",
    "    grid_classifier.fit(train_data.matrix, train_data.target)\n",
    "    predicted = grid_classifier.predict(test_data.matrix)\n",
    "\n",
    "    accuracy.append(metrics.accuracy_score(test_data.target, predicted))\n",
    "    precision.append(metrics.precision_score(test_data.target, predicted, average='macro', zero_division=0))\n",
    "    recall.append(metrics.recall_score(test_data.target, predicted, average='macro', zero_division=0))\n",
    "    f1.append(metrics.f1_score(test_data.target, predicted, average='macro', zero_division=0))\n",
    "\n",
    "    print(f'Finished {name} in {time.perf_counter() - start_time:.2f} seconds. Accuracy: {accuracy[-1]}', flush=True)\n",
    "    print(f'Best parameters: {grid_classifier.best_params_}')\n",
    "    \n",
    "    # ckpt_name = cache_path / f'{name}_{accuracy}.pkl'\n",
    "    # names = list(cache_path.glob(f'{name}_*.pkl'))\n",
    "    # if len(names) and accuracy > names[-1].split('_')[-1]:\n",
    "    #     with ckpt_name.open() as file_descr:\n",
    "    #         pickle.dump(grid_classifier, file_descr)\n",
    "\n",
    "    # plot_confusion_matrix(name, test_data.target, predicted, corpus_target)\n",
    "\n",
    "metrics_frame = pd.DataFrame({\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}, index=[name for name, _, _ in models])\n",
    "\n",
    "metrics_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model in terms of accuracy is a LinearSVC. This is a rather unexpected result, since the SVC algorithm well separates spaced clusters. This means that the preprocessing used to get the corpus embeddings reduced redundant and common words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I faced a problem with ploting confusion matrix. It takes >20h on M1 chip, so that option is turned off."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8378c2eac6e99e6afd3bc83cd2d86f951c26788ceeb3cc02a7a609063a83c45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
