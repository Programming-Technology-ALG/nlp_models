{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = Path('../cache/news_6771.json')\n",
    "with news_path.open(encoding=\"UTF-8\") as f:\n",
    "    news = json.load(f).get('catalog')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexgiving/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and lemmatize it with pymystem3 and remove stopwords and punctuation symbols from it\n",
    "    \"\"\"\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if \\\n",
    "                    token not in russian_stopwords \\\n",
    "                    and token != \" \" \\\n",
    "                    and token.strip() not in punctuation \\\n",
    "                    and token.strip() not in ['\\n', '\\t', '\\r']\n",
    "            ]\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from typing import Optional\n",
    "\n",
    "class CorpusStructure:\n",
    "    corpus: list\n",
    "    target: list\n",
    "    vectorizer: CountVectorizer\n",
    "    matrix: Optional[list] = None\n",
    "\n",
    "    def __init__(self, corpus: list, targets: list, vectorizer: CountVectorizer) -> None:\n",
    "        self._corpus = corpus\n",
    "        self._target = targets\n",
    "        self._vectorizer = vectorizer\n",
    "        self._matrix = None\n",
    "\n",
    "    @property\n",
    "    def corpus(self) -> list:\n",
    "        return self._corpus\n",
    "\n",
    "    @property\n",
    "    def target(self) -> list:\n",
    "        return self._target\n",
    "\n",
    "    @property\n",
    "    def corpus_len(self) -> int:\n",
    "        return len(self._corpus)\n",
    "\n",
    "    @property\n",
    "    def target_len(self) -> int:\n",
    "        return len(self._target)\n",
    "\n",
    "    @property\n",
    "    def matrix(self):\n",
    "        if self._matrix == None:\n",
    "            self._matrix = self._vectorizer.transform(self._corpus)\n",
    "        return self._matrix.toarray()\n",
    "\n",
    "    def transform(self):\n",
    "        return self._vectorizer.transform(self._corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(corpus: list, targets: list, vectorizer: CountVectorizer, test_size: float = 0.2) -> tuple:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(corpus, targets, test_size=test_size, random_state=42)\n",
    "    return CorpusStructure(X_train, y_train, vectorizer), CorpusStructure(X_test, y_test, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_corpus(news: list, n: int = -1, shuffle=True):\n",
    "    corpus_text = []\n",
    "    corpus_target = []\n",
    "    n = len(news) if n == -1 else n\n",
    "    if shuffle: random.shuffle(news)\n",
    "    for article in news[:n]:\n",
    "        corpus_text.append(preprocess_text(article.get('text')))\n",
    "        corpus_target.append(article.get('category'))\n",
    "    return corpus_text, corpus_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus_text, corpus_target = get_corpus(news)\n",
    "vectorizer.fit(corpus_text)\n",
    "\n",
    "train_data, test_data = train_test_split(corpus_text, corpus_target, vectorizer, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\"\"\"Module for grid search and classification of the data\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "models = [\n",
    "    (\n",
    "        'LogisticRegression',\n",
    "        LogisticRegression(),\n",
    "        {\n",
    "            'penalty'   : ['l1','l2'], \n",
    "            'solver'    : ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "            'C'         : [ 0.1, 1, 10],\n",
    "            'max_iter'  : [1000]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'MultinomialNB',\n",
    "        MultinomialNB(),\n",
    "        {\n",
    "            'alpha': [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'LinearSVC',\n",
    "        LinearSVC(),\n",
    "        {\n",
    "            'penalty'   : ['l1','l2'], \n",
    "            'loss'      : ['hinge', 'squared_hinge'],\n",
    "            'C'         : [ 0.1, 1, 10],\n",
    "            'max_iter'  : [1000]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'SGDClassifier',\n",
    "        SGDClassifier(),\n",
    "        {\n",
    "            'penalty'       : ['l1','l2'],\n",
    "            'alpha'         : [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100, 1000],\n",
    "            'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "            'max_iter'      : [1000],\n",
    "            'loss'          : ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'RandomForestClassifier',\n",
    "        RandomForestClassifier(),\n",
    "        {\n",
    "            'n_estimators'  : [100, 500], \n",
    "            'criterion'     : ['gini', 'entropy', 'log_loss'],\n",
    "            'max_features'  : ['sqrt', 'log2'],\n",
    "            'max_depth'     : [3, 5, 10],\n",
    "            'min_samples_split' : [2, 5, 10],\n",
    "            'min_samples_leaf'  : [1, 2, 5]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'KNeighborsClassifier',\n",
    "        KNeighborsClassifier(),\n",
    "        {\n",
    "            'weights' : ['uniform', 'distance'],\n",
    "            'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            'n_neighbors': [3, 5, 10],\n",
    "            'p': [1, 2]\n",
    "        }\n",
    "    ),\n",
    "    (\n",
    "        'DecisionTreeClassifier',\n",
    "        DecisionTreeClassifier(),\n",
    "        {\n",
    "            'criterion'     : ['gini', 'entropy', 'log_loss'],\n",
    "            'max_features'  : ['sqrt', 'log2']\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(name: str, test: list, predicted: list, target: list):\n",
    "    disp = metrics.ConfusionMatrixDisplay(metrics.confusion_matrix(test, predicted, labels=target))\n",
    "    disp.plot()\n",
    "    disp.ax_.set_title(f'{name} Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for i, (name, model, params) in enumerate(models):\n",
    "    grid_classifier = GridSearchCV(model,\n",
    "                                    params,\n",
    "                                    cv=3,\n",
    "                                    scoring='accuracy',\n",
    "                                    verbose=0,\n",
    "                                    error_score=0,\n",
    "                                    n_jobs=-1\n",
    "                                    )\n",
    "    grid_classifier.fit(train_data.matrix, train_data.target)\n",
    "    predicted = grid_classifier.predict(test_data.matrix)\n",
    "\n",
    "    accuracy.append(metrics.accuracy_score(test_data.target, predicted))\n",
    "    precision.append(metrics.precision_score(test_data.target, predicted, average='macro', zero_division=0))\n",
    "    recall.append(metrics.recall_score(test_data.target, predicted, average='macro', zero_division=0))\n",
    "    f1.append(metrics.f1_score(test_data.target, predicted, average='macro', zero_division=0))\n",
    "\n",
    "    # plot_confusion_matrix(name, test_data.target, predicted, corpus_target)\n",
    "\n",
    "metrics_frame = pd.DataFrame({\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}, index=[name for name, _, _ in models])\n",
    "\n",
    "metrics_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8378c2eac6e99e6afd3bc83cd2d86f951c26788ceeb3cc02a7a609063a83c45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
